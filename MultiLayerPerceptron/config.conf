#number of hidden layers
global hidden_layers 	  = 1; 

#max tolerance error to neural network output
global max_error			  = 0.1;

#learning factor should be small
global learning_rate     = 0.0005;

#array with number of units(without bias) on each layer going from input layer to output layer
global units_per_layer      = [2, 2, 1];

#value of 0 for incremental and 1 for batch
global method 			  = 0;

#initialize weights with 0  random 1 fanning 2 normalized inputs
global weight_init_method   = 0;

#When init method is random interval would be [-randAbsolut, randAbsolut]
global rand_absolut        = 0.5;

#0 for no momentum, (0,1] for momentum
global momentum_alpha           = 0;

#activation function for layer 0 tanh, 1 exp, 2 linear
global function_type = 0;

#1 for adaptive learning, 0 for non-adaptive
global adaptive_learning = 0;

########################################################
#            Adaptative Learning Rate                  #
#                                                      #
#           { +a    if delta_E < 0 in the last k steps #
# delta_n = { -bn   if delta_E > 0                     #
#           { 0     else                               #
#                                                      #
global learning_rate_k = 10;
global learning_rate_a = 0.1;
global learning_rate_b = 0.1;